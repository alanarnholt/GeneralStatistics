---
title: "*Train* and *Test* Partitions"
author: "Alan T. Arnholt"
date: 'Last knit on `r format(Sys.time(), "%B %d, %Y at %X")`'
output: 
  bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA, fig.align = 'center', fig.height = 5, fig.width = 5, 
                      prompt = TRUE, highlight = TRUE, tidy = FALSE, warning = FALSE, 
                      message = FALSE)
library(mdsr)
HELPrct <- na.omit(HELPrct)
```

# Partitions

When building a model used for prediction, one measure of the model's usefulness is how well it performs on unseen (future) data.  One method to evaluate a model on unseen data is to partition the data into a *`training set`* and a *`testing set`*.  The *`training set`* is the data set one uses to build the model.  The *`test set`* is the data one uses to evaluate the model's predictions.  Three approaches are used to create a *`training set`* and a *`test set`*.

The data set `HELPrct` after removing rows with missing values from the `mdsr` package is used to illustrate the three approaches with a 70/30 split of values.  That is, roughly 70% of the values will be used for the *`training set`* and the remainder will be used for the *`test set`*.  Note that there are `r nrow(HELPrct)` rows of data in `HELPrct` with complete information (no missing values), so a 70/30 split should result in roughly $`r nrow(HELPrct)`\times 0.70 \approx `r round(nrow(HELPrct)*.7,0)`$ values in the *`training set`* and the remaining `r nrow(HELPrct) - round(nrow(HELPrct)*.7,0)` values in the *`test set`*. 

* Using the `sample()` function (method 1):

```{r}
library(mdsr)
HELPrct <- na.omit(HELPrct)
set.seed(123)
trainIndex1 <- sample(x = c(TRUE, FALSE), size = nrow(HELPrct), replace = TRUE, prob = c(0.7, 0.3))
testIndex1 <- (!trainIndex1)
trainData1 <- HELPrct[trainIndex1, ]
testData1 <- HELPrct[testIndex1, ]
dim(HELPrct)
dim(trainData1)
dim(testData1)
# Train Fraction
dim(trainData1)[1]/dim(HELPrct)[1]
# Test Fraction
dim(testData1)[1]/dim(HELPrct)[1]
```

* Using the `sample()` function (method 2):

```{r}
set.seed(123)
trainIndex2 <- sample(x = 1:nrow(HELPrct), size = round(0.70*nrow(HELPrct), 0), replace = FALSE)
trainData2 <- HELPrct[trainIndex2, ]
testData2 <- HELPrct[-trainIndex2, ]
dim(trainData2)
dim(testData2)
# Train Fraction
dim(trainData2)[1]/dim(HELPrct)[1]
# Test Fraction
dim(testData2)[1]/dim(HELPrct)[1]
```

* Using `runif()`:

```{r}
set.seed(123)
HELPrct$Partition <- runif(dim(HELPrct)[1])
trainData3 <- subset(HELPrct, HELPrct$Partition > 0.3)
testData3 <- subset(HELPrct, HELPrct$Partition <= 0.3)
dim(trainData3)
dim(testData3)
# Train Fraction
dim(trainData3)[1]/dim(HELPrct)[1]
# Test Fraction
dim(testData3)[1]/dim(HELPrct)[1]
```

* Using the `createDataPartition` from the `caret` package:

```{r}
library(caret)  # load the caret package
set.seed(123)
trainIndex4 <- createDataPartition(y = HELPrct$homeless, p = .7, 
                                  list = FALSE, 
                                  times = 1)
trainData4 <- HELPrct[trainIndex4, ]
testData4 <- HELPrct[-trainIndex4, ]
dim(trainData4)
dim(testData4)
# Train Fraction
dim(trainData4)[1]/dim(HELPrct)[1]
# Test Fraction
dim(testData4)[1]/dim(HELPrct)[1]
```

**Note:**  When the `y` argument to `createDataPartition` is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data. 

## Building a model

In this section, a model is built using the *`training set`* to predict whether a person is `homeless`.

```{r}
model.null <- glm(homeless ~ 1, data = trainData4, family = binomial)
model.full <- glm(homeless ~ ., data = trainData4, family = binomial)
AICmod <- step(model.null, scope = list(upper = model.full), direction = "both", data = trainData4)
summary(AICmod)
```
### Pseudo $R^2$

One can think of the pseudo $R^2$ for logistic regression as the analog to $R^2$ used in linear regression.  Pseudo $R^2$ measures how much of the deviance is explained by the model, where $R^2$ measures the variability explained by a linear model.

```{r}
resid.dev <- summary(AICmod)$deviance
resid.dev
null.dev <- summary(AICmod)$null.deviance
null.dev
PR2 <- 1 - (resid.dev / null.dev)
PR2 # Pseudo R squared
```

The model only explains `r round(PR2*100,2)`% of the deviance in the training data.  This will likely be less for the test data suggesting there are likely other factors that have not been measured that predict homelessness.

```{r}
testmod <- glm(homeless ~ i1 + sex + g1b + pss_fr + dayslink + sexrisk, family = binomial, data = testData4)
resid.dev <- summary(testmod)$deviance
resid.dev
null.dev <- summary(testmod)$null.deviance
null.dev
PR2 <- 1 - (resid.dev / null.dev)
PR2 # Pseudo R squared
PM <- predict(testmod, type = "response")
pred <- ifelse(PM > 0.5, "homeless", "housed")
T1 <- table(pred, testData4$homeless)
T1
Accuracy <- sum(diag(T1))/sum(T1)
Accuracy
# Using caret
confusionMatrix(data = pred, reference = testData4$homeless)
```
See [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix) on Wikipedia.
